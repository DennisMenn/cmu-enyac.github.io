{
	"title": "HyperPower: Power- and Memory-Constrained Hyper-Parameter Optimization for Neural Networks",
    "picture": "./img/date2018.png",
	"content": "In this work, we are interested in finding energy-efficient neural networks. While selecting the hyper-parameters of Neural Networks (NNs) has been so far treated as an art, the emergence of more complex, deeper architectures poses increasingly more challenges to designers and Machine Learning (ML) practitioners, especially when power and memory constraints need to be considered. In this work, we propose HyperPower, a framework that enables efcient Bayesian optimization and random search in the context of power-and memory-constrained hyperparameter optimization for NNs running on a given hardware platform. HyperPower is the rst work (i) to show that power limitations can be used as low-cost, a priori known constraints, and (ii) to propose predictive models for the power and memory of NNs executing on GPUs. Thanks to HyperPower, the number of function evaluations and the best test error achieved by a constraint-unaware method are reached up to 112.99× and 30.12× faster, respectively, while never considering invalid congurations. HyperPower signicantly speeds up the hyperparameter optimization, achieving up to 57.20× more function evaluations compared to constraint-unaware methods for a given time interval, effectively yielding signicant accuracy improvements by up to 67.6%."
}